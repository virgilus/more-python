{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9014a07e",
   "metadata": {},
   "source": [
    "# üîÄ Multiprocessing\n",
    "\n",
    "## üìñ Introduction\n",
    "\n",
    "This chapter is the second part of the multithreading chapter.\n",
    "\n",
    "Now that you know what is the GIL and how to use threads, let's talk about the ways Python is able to run code in parallel.\n",
    "\n",
    "<img src='files/multithreading_vs_multiprocessing.png' alt='Multithreading vs Multiprocessing diagram' width='600' source=\"miro.medium.com\">\n",
    "\n",
    "## üì¶ The Multiprocessing Module\n",
    "\n",
    "Python's `multiprocessing` module allows you to create multiple processes, each with its own Python interpreter and memory space. This is particularly useful for CPU-bound tasks that can benefit from parallel execution.\n",
    "\n",
    "### üéØ Concurrent.futures\n",
    "\n",
    "Remember the `concurrent.futures` module, which provides a high-level interface for asynchronously executing callables ?\n",
    "\n",
    "In the last chapter, we used `concurrent.futures.ThreadPoolExecutor` to run tasks concurrently using threads.\n",
    "\n",
    "Now, we can use `concurrent.futures.ProcessPoolExecutor` to run tasks concurrently using processes.\n",
    "\n",
    "The syntax is very similar to what we used with threads, so you can easily switch between the two.\n",
    "\n",
    "Let's take the same code we used in the last chapter and modify it to use `ProcessPoolExecutor` instead of `ThreadPoolExecutor`.\n",
    "\n",
    "Also this time, we will use a the function `os.getpid()` to print the process ID of each task, so we can see which process is running which task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd35a3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With ThreadPoolExecutor and os.getpid()\n",
    "\n",
    "import threading\n",
    "import concurrent.futures\n",
    "import time\n",
    "import os\n",
    "\n",
    "print(f\"Main process {os.getpid()}\")\n",
    "\n",
    "def task(n):\n",
    "    print(f\"{\" \" * n}Task {n} starting on thread {threading.current_thread().name}, process {os.getpid()}\")\n",
    "    time.sleep(n)\n",
    "    print(f\" {\" \" * n}Task {n} completed on thread {threading.current_thread().name}, process {os.getpid()}\")\n",
    "    return n * n\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:\n",
    "    results = executor.map(task, [1, 2, 3, 4, 5])\n",
    "\n",
    "print(\"Results:\", list(results))\n",
    "print(\"Main thread finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6777a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's try with ProcessPoolExecutor.\n",
    "\n",
    "import threading\n",
    "import concurrent.futures\n",
    "import time\n",
    "import os\n",
    "\n",
    "print(f\"Main process {os.getpid()}\")\n",
    "def task(n):\n",
    "    print(f\"{\" \" * n}Task {n} starting on process {threading.current_thread().name}, process {os.getpid()}\")\n",
    "    time.sleep(n)\n",
    "    print(f\" {\" \" * n}Task {n} completed on process {threading.current_thread().name}, process {os.getpid()}\")\n",
    "    return n * n\n",
    "with concurrent.futures.ProcessPoolExecutor(max_workers=3) as executor:\n",
    "    results = executor.map(task, [1, 2, 3, 4, 5])\n",
    "print(\"Results:\", list(results))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd30692",
   "metadata": {},
   "source": [
    "- When using 'ThreadPoolExecutor', the process ID is the the same for all tasks, since they are all running in the same process.\n",
    "\n",
    "- When using 'ProcessPoolExecutor', each task will run in a separate OS process, so the process ID is different for each task. If the print statements are not clear, it's because Jupyter Notebook is not designed to show output from multiple processes in a clear way.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc9de6f",
   "metadata": {},
   "source": [
    "## ‚ö° Speed Comparison CPU Task Bound\n",
    "\n",
    "Let's compare the performance of using threads without threads and using processes for a CPU-bound task.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4badce0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without any threading or processing\n",
    "\n",
    "import time\n",
    "\n",
    "def cpu_bound_task(n):\n",
    "    print(f\"Starting task {n} in process {os.getpid()}\")\n",
    "    count = 0\n",
    "    for i in range(10**7):\n",
    "        count += i % n\n",
    "    print(f\"Completed task {n} in process {os.getpid()}\")\n",
    "    return count\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for n in range(1, 6):\n",
    "    cpu_bound_task(n)\n",
    "end = time.time()\n",
    "print(f\"Total time without any executor: {end - start} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd305c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With Multithreading (ThreadPoolExecutor)\n",
    "\n",
    "import time\n",
    "\n",
    "def cpu_bound_task(n):\n",
    "    print(f\"Starting task {n} in process {os.getpid()}\")\n",
    "    count = 0\n",
    "    for i in range(10**7):\n",
    "        count += i % n\n",
    "    print(f\"Completed task {n} in process {os.getpid()}\")\n",
    "    return count\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:\n",
    "    executor.map(cpu_bound_task, range(1, 6))\n",
    "    \n",
    "end = time.time()\n",
    "print(f\"Total time ThreadPoolExecutor: {end - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2bc598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With Multiprocessing (ProcessPoolExecutor)\n",
    "\n",
    "import time\n",
    "\n",
    "def cpu_bound_task(n):\n",
    "    print(f\"Starting task {n} in process {os.getpid()}\")\n",
    "    count = 0\n",
    "    for i in range(10**7):\n",
    "        count += i % n\n",
    "    print(f\"Completed task {n} in process {os.getpid()}\")\n",
    "    return count\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor(max_workers=6) as executor:\n",
    "    executor.map(cpu_bound_task, range(1, 6))\n",
    "    \n",
    "end = time.time()\n",
    "print(f\"Total time with ProcessPoolExecutor: {end - start} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a79148",
   "metadata": {},
   "source": [
    "### üìä Conclusion\n",
    "\n",
    "- For a CPU-bound task, using only one process is the same, or sometimes faster, than using threads, because of the GIL.\n",
    "\n",
    "- With `ProcessPoolExecutor`, we can see a significant speedup, because each process can run on a separate CPU core, bypassing the GIL limitation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9246da8a",
   "metadata": {},
   "source": [
    "### üí™ Exercice\n",
    "\n",
    "CPython comes now as two versions: one with GIL and one without GIL (called Gilectomy).\n",
    "\n",
    "Try to reproduce this experiment using the Gilectomy version of Python and see how the results change.\n",
    "\n",
    "https://medium.com/sdg-group/exploring-pythons-gil-single-multithreading-vs-multiprocessing-and-the-impact-of-gil-removal-ee8b6dd610f4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218407fa",
   "metadata": {},
   "source": [
    "## üöÄ Other ways to run code in parallel\n",
    "\n",
    "Using the library `multiprocessing` is not the only way to run code in parallel in Python. Some librairies provide their own way to do so.\n",
    "\n",
    "### üî¢ NumPy / SciPy\n",
    "\n",
    "NumPy is fast because heavy work happens in compiled C/Fortran, not Python. Those low-level libraries (BLAS, LAPACK, MKL, OpenBLAS) often use multiple CPU cores automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d32cff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# np.show_config() # Check if NumPy is linked against a multi-threaded BLAS implementation\n",
    "\n",
    "A = np.random.rand(5000, 5000)\n",
    "B = np.random.rand(5000, 5000)\n",
    "\n",
    "# Matrix multiplication\n",
    "C = A @ B  # Check your CPU usage during this operation !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9a5241",
   "metadata": {},
   "source": [
    "### ‚ö° Numba\n",
    "\n",
    "Numba is a just-in-time compiler for Python that translates a subset of Python and NumPy code into fast machine code. It can automatically parallelize certain operations using multiple CPU cores.\n",
    "\n",
    "### ü§ñ PyTorch / TensorFlow / JAX\n",
    "\n",
    "These are parallel by design. They use C++ backends, release the GIL and run on GPU and CPU.\n",
    "\n",
    "### üìä Dask\n",
    "\n",
    "Dask is a flexible parallel computing library for analytics. It allows you to scale your computations from a single machine to a cluster of machines. Dask can parallelize NumPy, Pandas, and other operations easily.\n",
    "\n",
    "### üêª‚Äç‚ùÑÔ∏è Polars\n",
    "\n",
    "Polars is a fast DataFrame library implemented in Rust. It is designed for high performance and can utilize multiple CPU cores for data processing tasks. The only downside is that the API is not exactly the same as Pandas and you may need to adapt your code.\n",
    "\n",
    "### ‚ö° Pyspark\n",
    "\n",
    "PySpark is the Python API for Apache Spark, a distributed computing framework. It allows you to process large datasets in parallel across a cluster of machines. PySpark is particularly useful for big data applications. When using on a single machine, it can still utilize multiple CPU cores for parallel processing, but it may have more overhead compared to other libraries like Dask or Polars for smaller datasets. So, unless your plan is to scale to a cluster later, prefer Dask or Polars for single-machine parallelism.\n",
    "\n",
    "### ü¶Ä Rust code in Python\n",
    "\n",
    "If you need extreme performance and parallelism, you can write performance-critical parts of your code in Rust and call them from Python using libraries like `PyO3`. Rust has excellent support for concurrency and parallelism, allowing you to leverage multiple CPU cores effectively."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
